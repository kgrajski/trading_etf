# v0.1 Lessons Learned — Single-Agent AI Analyst

**Date:** 2026-02-17
**Tag:** `v0.1-analyst-freeze`

---

## What We Built

A weekly ETF trading system with a single-agent AI analyst overlay:

1. **Quantitative pipeline** — data fetch, feature engineering, backtesting, trade
   candidate generation (~2,200 ETFs, mean-reversion strategy)
2. **AI analyst** — LangGraph workflow with 5 sequential nodes: load → fetch_news →
   analyze_themes → analyze_symbols → review_and_refine
3. **Interactive dashboard** — HTML inspector with candlestick charts, Bollinger Bands,
   trade levels, and AI analysis overlays
4. **Instrumentation** — MLflow tracking, token/cost metrics, latency breakdown

The system runs end-to-end in ~10 minutes and costs ~$0.02 per run (32 candidates).

---

## What Worked

### 1. The Pipeline Architecture
The sequential weekly pipeline (`weekly_update.sh`) is reliable and easy to reason about.
Each step produces artifacts that the next step consumes. Re-running any step is safe.

### 2. Mean-Reversion Strategy
The quantitative strategy is sound. Bottom-5% ETFs with >2% weekly loss, 1-week hold,
16% stop loss, 10% profit target. Backtesting validates the approach across multiple
market regimes.

### 3. Interactive Inspector
The Plotly-based HTML dashboard is genuinely useful. Candlestick charts with Bollinger
Bands, hover data, zoom, and clickable navigation between market overview and symbol
detail. The AI overlays (flags, theme modals, symbol analysis panels) add real value
to the reading experience.

### 4. Multi-Model Routing
Using GPT-4o for synthesis/review and GPT-4o-mini for per-symbol analysis is a good
cost/quality trade-off. Theme-level analysis (where all information must be synthesized)
benefits from the stronger model. Per-symbol analysis (more formulaic) is adequate with
the cheaper model — though with caveats (see below).

### 5. Instrumentation
MLflow tracking, token counting, cost estimation, and latency breakdown per node are
invaluable for understanding where time and money go. The instrumentation report makes
optimization decisions data-driven.

---

## What Didn't Work / Key Challenges

### 1. Generic and Hallucinated Analysis
**The #1 problem.** When news for a specific ETF is weak or absent, GPT-4o-mini fills
the gap with plausible-sounding but ungrounded narrative. Examples:
- "Key Concern: Continued decline in regional banks due to unknown risks" — says nothing
- BPAY (FinTech ETF) linked to consumer discretionary news — wrong sector entirely
- Vague "general market sentiment" or "profit-taking" without specifics

**Root cause:** The model is optimized for fluency, not epistemic honesty. Without
strong guardrails, it will always produce confident-sounding text, even when it has
no evidence.

**Partial mitigations attempted:**
- Evidence quality guard (`strong/moderate/weak/none`) in output schema
- Explicit prompt instructions: "If no evidence, say so"
- These help but don't fully solve it — GPT-4o-mini still hedges rather than admitting
  ignorance

### 2. Sector Inconsistency
ETFs covering the same sector (e.g., VFH, IYF, IYG, XLF for U.S. Financials) get
independent searches and independent analysis. This produces embarrassing contradictions:
one says "yield curve normalization is bullish" while the neighbor says "AI disruption
is the concern."

**Root cause:** No concept of sector grouping. Each symbol is analyzed in isolation.
The thematic analysis partially catches this but the per-symbol analysis doesn't benefit.

### 3. Missing Quantitative Grounding
The AI analyst operates purely on news text. It has no access to:
- Beta (is this a market-wide move or ETF-specific?)
- Alpha residual (how much of the drop is unexplained by market movement?)
- Technical levels (price vs. 50-DMA, 200-DMA, RSI)
- Volume profile (was this high-volume panic or normal flow?)

A human analyst would look at these *first* before reading news. Our AI analyst reads
news in a vacuum.

### 4. Prompt Sensitivity and Bias
- We accidentally hardcoded "AI disruption" as an example in the analysis prompts,
  permanently biasing the model toward AI-related explanations
- Small prompt changes produce outsized output differences
- The structured 7-dimension checklist improved consistency but the model still picks
  the "easiest" dimensions to write about rather than the most relevant

### 5. Dual-Script Confusion
Having `21b-gen-new-trades.py` (production) and `19.3-gen-trades.py` (research) with
different strategy parameters created confusion:
- Different candidate sets from the same underlying data
- Manual parameter alignment required
- Unclear which dashboard is the "real" one

### 6. Single-Agent Sequential Bottleneck
The current architecture is a single chain: one agent, five nodes, strictly sequential.
This means:
- The news search (36% of total time) blocks everything
- The per-symbol analysis (32 sequential LLM calls) is the longest step
- No parallelism within the agent
- No way for one analytical perspective (quant) to inform another (news) mid-flight

### 7. Reviewer Section UX
- "NEEDS_REVISION" label was confusing to readers
- Quality score (6/10) is opaque without context
- "Missed Signals" heading is ambiguous
- The reviewer model's adjustments are not clearly surfaced

---

## Key Insights

1. **LLMs are better at synthesis than discovery.** They can organize and narrate
   information you give them. They're poor at finding information on their own and
   terrible at admitting when they can't find it.

2. **Quantitative grounding is essential.** The alpha/beta decomposition insight:
   if beta explains the drop, the LLM should say "this is a market move, don't
   over-explain it." Without this, the LLM invents ETF-specific narratives for
   market-wide movements.

3. **Sector is the right unit of analysis** for news, not individual ETFs. Search
   once per sector, analyze once per sector, then note per-ETF nuances.

4. **Evidence quality matters more than analysis quality.** A system that honestly
   says "no specific evidence found" is more useful than one that produces fluent
   but ungrounded analysis.

5. **Multi-agent architecture is the natural next step.** Different analytical
   perspectives (quant, news, synthesis, review) are genuinely different skills
   that benefit from different tools, prompts, and potentially different models.

---

## Metrics Snapshot (v0.1, 32 candidates, 2026-02-17)

| Metric | Value |
|--------|-------|
| Total runtime | 310 seconds |
| LLM time | 197s (64%) |
| Tool (search) time | 113s (36%) |
| Total tokens | 69,234 |
| Total cost | $0.014 |
| Cost per symbol | $0.0004 |
| LLM calls | 34 |
| Tool calls | 33 |
| Models | GPT-4o-mini (analyst), GPT-4o (themes + review) |
